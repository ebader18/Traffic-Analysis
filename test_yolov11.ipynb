{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track all ojects using bounding box in video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLO11n model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Run inference with the YOLO11n model on the 'bus.jpg' image\n",
    "results = model.track(\"video_001.mp4\", save=True, show=True, tracker='bytetrack.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track all ojects using segmentation in video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11x-seg.pt\")  # load an official model\n",
    "# model = YOLO(\"path/to/best.pt\")  # load a custom model\n",
    "\n",
    "# Predict with the model\n",
    "results = model.track(source=\"video_001.mp4\", save=True)  # predict on an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track all objeects using segmentation once image at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "filename = 'video_001'\n",
    "model = YOLO(\"yolo11x-seg.pt\")  # segmentation model\n",
    "cap = cv2.VideoCapture(f'{filename}.mp4')\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "out = cv2.VideoWriter(f'{filename}_x_instance-segmentation-object-tracking.mp4', cv2.VideoWriter_fourcc(*\"H265\"), fps, (w, h))\n",
    "\n",
    "idx_frame = 0\n",
    "while True:\n",
    "    ret, im0 = cap.read()\n",
    "    idx_frame += 1\n",
    "    if not ret or idx_frame == -1:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    annotator = Annotator(im0, line_width=2)\n",
    "\n",
    "    results = model.track(im0, persist=True)\n",
    "\n",
    "    if results[0].boxes.id is not None and results[0].masks is not None:\n",
    "        masks = results[0].masks.xy\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        track_cls = [results[0].names[i] for i in results[0].boxes.cls.int().cpu().tolist()]\n",
    "\n",
    "        #for mask, track_id in zip(masks, track_ids):\n",
    "        for mask, track_cl in zip(masks, track_cls):\n",
    "            #color = colors(int(track_id), True)\n",
    "            color = colors(0, True)\n",
    "            txt_color = annotator.get_txt_color(color)\n",
    "            #annotator.seg_bbox(mask=mask, mask_color=color, label=str(track_id), txt_color=txt_color)\n",
    "            annotator.seg_bbox(mask=mask, mask_color=color, label=str(track_cl), txt_color=txt_color)\n",
    "\n",
    "    out.write(im0)\n",
    "    cv2.imshow(f'{filename}video_002_trimmed_instance-segmentation-object-tracking', im0)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track specific object using segmentation one image at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "from queue import Queue\n",
    "import numpy as np\n",
    "\n",
    "class ObjectTracker:\n",
    "    def __init__(self, model_path=\"yolo11x-seg.pt\", track_class=\"car\"):\n",
    "        \"\"\"\n",
    "        Initialize the ObjectTracker with the specified model and class to track.\n",
    "\n",
    "        :param model_path: Path to the YOLO model.\n",
    "        :param track_class: Name of the class to track (default is 'car').\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.track_class = track_class\n",
    "\n",
    "    def process_image(self, image):\n",
    "        \"\"\"\n",
    "        Process an image to detect and segment the specified object class.\n",
    "\n",
    "        :param image: The input image.\n",
    "        :return: A tuple containing the original image and a list of tracking results, where each result includes the object ID, the masked image, and the top-left coordinate.\n",
    "        \"\"\"\n",
    "        results = self.model.track(image, persist=True)\n",
    "        output_data = []\n",
    "\n",
    "        if results[0].boxes.id is not None and results[0].masks is not None:\n",
    "            masks = results[0].masks.xy\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            track_cls = [results[0].names[i] for i in results[0].boxes.cls.int().cpu().tolist()]\n",
    "\n",
    "            for mask, obj_id, track_cl in zip(masks, track_ids, track_cls):\n",
    "                if track_cl == self.track_class:\n",
    "                    # Apply the mask to the original image\n",
    "                    masked_image = self.apply_mask(image, mask)\n",
    "                    output_data.append({\n",
    "                        \"object_id\": obj_id,\n",
    "                        \"masked_image\": masked_image,\n",
    "                        \"top_left\": (0, 0)  # Always (0, 0) as we keep the original size\n",
    "                    })\n",
    "        return output_data\n",
    "\n",
    "    def apply_mask(self, image, mask):\n",
    "        \"\"\"\n",
    "        Apply the mask to the image while keeping the original size, \n",
    "        setting everything outside the mask to black.\n",
    "\n",
    "        :param image: The original image.\n",
    "        :param mask: The mask coordinates.\n",
    "        :return: The masked image with the same size as the original.\n",
    "        \"\"\"\n",
    "        # Create a blank mask of the same size as the image\n",
    "        mask_image = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        # Draw the polygon mask on the blank image\n",
    "        cv2.fillPoly(mask_image, [np.array(mask, dtype=np.int32)], 255)\n",
    "\n",
    "        # Apply the mask to the original image\n",
    "        masked_image = cv2.bitwise_and(image, image, mask=mask_image)\n",
    "\n",
    "        return masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 5 cars, 34.2ms\n",
      "Speed: 2.5ms preprocess, 34.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 30.6ms\n",
      "Speed: 1.0ms preprocess, 30.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "103 features detected in image #1\n",
      "118 features detected in image #2\n",
      "103 matches found before pruning\n",
      "41 matches found after pruning\n",
      "Fundamental matrix: [[ 8.2508e-07   0.0001212    -0.11456]\n",
      " [-0.00011997 -8.7346e-08     0.18098]\n",
      " [     0.1117    -0.18121           1]]\n",
      "Essential matrix: [[     6.2036      769.72      49.137]\n",
      " [    -761.92    -0.46854     -114.56]\n",
      " [    -44.667      119.06     0.74179]]\n",
      "Rotation matrix: [[    0.99998  -0.0034406  -0.0045346]\n",
      " [  0.0034556     0.99999   0.0032911]\n",
      " [  0.0045232  -0.0033067     0.99998]]\n",
      "translation matrix: [[    0.15263]\n",
      " [   0.059076]\n",
      " [   -0.98652]]\n",
      "\n",
      "0: 384x640 5 cars, 22.6ms\n",
      "Speed: 1.0ms preprocess, 22.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "118 features detected in image #1\n",
      "120 features detected in image #2\n",
      "118 matches found before pruning\n",
      "41 matches found after pruning\n",
      "Fundamental matrix: [[ 2.8507e-18   7.592e-17    0.083333]\n",
      " [-7.3361e-17 -2.9323e-19  1.0946e-13]\n",
      " [  -0.083333 -1.1175e-13           1]]\n",
      "Essential matrix: [[ 2.1434e-11  4.8215e-10       228.5]\n",
      " [ -4.659e-10 -1.5729e-12  -7.345e-11]\n",
      " [     -228.5  7.8054e-11           1]]\n",
      "Rotation matrix: [[          1 -6.9318e-15  -0.0021881]\n",
      " [ 6.4536e-15           1 -7.0406e-14]\n",
      " [  0.0021881  7.0067e-14           1]]\n",
      "translation matrix: [[-3.3004e-13]\n",
      " [         -1]\n",
      " [ 2.0392e-12]]\n",
      "\n",
      "0: 384x640 5 cars, 22.5ms\n",
      "Speed: 1.0ms preprocess, 22.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "120 features detected in image #1\n",
      "113 features detected in image #2\n",
      "120 matches found before pruning\n",
      "61 matches found after pruning\n",
      "Fundamental matrix: [[ 6.6932e-06 -0.00075165     0.75935]\n",
      " [ 0.00074085 -1.5029e-05     -1.0828]\n",
      " [   -0.77364      1.1196           1]]\n",
      "Essential matrix: [[     50.325     -4773.6     -108.55]\n",
      " [       4705     -80.616      748.97]\n",
      " [     107.83     -787.07     -2.0112]]\n",
      "Rotation matrix: [[    0.99998  -0.0026857  -0.0048696]\n",
      " [  0.0027038     0.99999   0.0037092]\n",
      " [  0.0048596  -0.0037223     0.99998]]\n",
      "translation matrix: [[    0.16231]\n",
      " [   0.020874]\n",
      " [   -0.98652]]\n",
      "\n",
      "0: 384x640 5 cars, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "113 features detected in image #1\n",
      "105 features detected in image #2\n",
      "113 matches found before pruning\n",
      "91 matches found after pruning\n",
      "Fundamental matrix: [[ 3.9067e-20 -4.7832e-18    0.041112]\n",
      " [ 5.1093e-18  2.5479e-20 -7.4074e-15]\n",
      " [  -0.041112  6.8847e-15  6.3238e-13]]\n",
      "Essential matrix: [[ 2.9374e-13 -3.0377e-11      112.73]\n",
      " [ 3.2448e-11  1.3667e-13  5.6281e-12]\n",
      " [    -112.73  -5.261e-12           0]]\n",
      "Rotation matrix: [[         -1 -9.6594e-14  1.2212e-15]\n",
      " [-9.6594e-14           1   5.573e-13]\n",
      " [-1.2212e-15   5.573e-13          -1]]\n",
      "translation matrix: [[ 4.9925e-14]\n",
      " [         -1]\n",
      " [-2.8784e-13]]\n",
      "\n",
      "0: 384x640 5 cars, 34.6ms\n",
      "Speed: 2.0ms preprocess, 34.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "105 features detected in image #1\n",
      "115 features detected in image #2\n",
      "105 matches found before pruning\n",
      "47 matches found after pruning\n",
      "Fundamental matrix: [[ 2.7416e-07  1.5378e-06  -0.0030566]\n",
      " [-1.1696e-06  6.0695e-08   0.0015108]\n",
      " [  0.0019759  -0.0021427           1]]\n",
      "Essential matrix: [[     2.0614      9.7665     -2.3837]\n",
      " [    -7.4279     0.32558     -1.5502]\n",
      " [     3.3978      2.0278    0.087718]]\n",
      "Rotation matrix: [[    0.99366    -0.11159     0.01396]\n",
      " [    0.10908     0.98657     0.12157]\n",
      " [  -0.027338    -0.11927     0.99248]]\n",
      "translation matrix: [[    0.20111]\n",
      " [   -0.36078]\n",
      " [   -0.91071]]\n",
      "\n",
      "0: 384x640 6 cars, 63.5ms\n",
      "Speed: 2.0ms preprocess, 63.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "115 features detected in image #1\n",
      "141 features detected in image #2\n",
      "115 matches found before pruning\n",
      "59 matches found after pruning\n",
      "Fundamental matrix: [[ 6.7232e-08 -2.8834e-05    0.030762]\n",
      " [ 2.8828e-05 -2.9178e-09   -0.038808]\n",
      " [  -0.031231    0.038361           1]]\n",
      "Essential matrix: [[     0.5055     -183.12    -0.68333]\n",
      " [     183.08   -0.015652      38.302]\n",
      " [   0.088875     -39.382    -0.15111]]\n",
      "Rotation matrix: [[    0.99998    0.001805  -0.0055917]\n",
      " [ -0.0017824     0.99999   0.0040445]\n",
      " [  0.0055989  -0.0040344     0.99998]]\n",
      "translation matrix: [[    0.21026]\n",
      " [-0.00010595]\n",
      " [   -0.97765]]\n",
      "\n",
      "0: 384x640 6 cars, 64.6ms\n",
      "Speed: 1.0ms preprocess, 64.6ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "141 features detected in image #1\n",
      "118 features detected in image #2\n",
      "141 matches found before pruning\n",
      "56 matches found after pruning\n",
      "Fundamental matrix: [[-6.6374e-09 -1.4243e-05    0.014435]\n",
      " [ 1.4589e-05   8.726e-08   -0.021186]\n",
      " [  -0.014915    0.020309           1]]\n",
      "Essential matrix: [[  -0.049906     -90.454     -2.6332]\n",
      " [     92.652     0.46807      16.026]\n",
      " [     2.2712     -16.081   -0.074083]]\n",
      "Rotation matrix: [[    0.99998   0.0027127  -0.0048739]\n",
      " [ -0.0026934     0.99999   0.0039594]\n",
      " [  0.0048845  -0.0039462     0.99998]]\n",
      "translation matrix: [[    0.17511]\n",
      " [   0.024221]\n",
      " [   -0.98425]]\n",
      "\n",
      "0: 384x640 6 cars, 62.4ms\n",
      "Speed: 2.0ms preprocess, 62.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "118 features detected in image #1\n",
      "130 features detected in image #2\n",
      "118 matches found before pruning\n",
      "50 matches found after pruning\n",
      "Fundamental matrix: [[ 4.7056e-07 -3.9491e-05    0.034872]\n",
      " [ 3.9212e-05 -2.9991e-07   -0.055897]\n",
      " [  -0.036099    0.056336           1]]\n",
      "Essential matrix: [[      3.538      -250.8     -18.851]\n",
      " [     249.02     -1.6088      44.157]\n",
      " [     19.615     -45.885   -0.076008]]\n",
      "Rotation matrix: [[    0.99998   0.0041763  -0.0041955]\n",
      " [ -0.0041729     0.99999  0.00080761]\n",
      " [  0.0041989 -0.00079008     0.99999]]\n",
      "translation matrix: [[      0.179]\n",
      " [    0.07473]\n",
      " [   -0.98101]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "sys.path.append(os.path.abspath('../Live-Pose-Estimator'))\n",
    "from pose_estimator import pose_estimator\n",
    "\n",
    "FILENAME = 'video_002'\n",
    "TRACK_CLASS = 'car'\n",
    "\n",
    "cap = cv2.VideoCapture(f'{FILENAME}.mp4')\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "tracker = ObjectTracker(track_class=\"car\")  # Initialize tracker for cars\n",
    "\n",
    "\n",
    "# Principal point (assuming the center of the image)\n",
    "cx, cy = w / 2, h / 2\n",
    "# Convert FOV from degrees to radians\n",
    "fov_x, fov_y = 70, 50\n",
    "fov_x_rad, fov_y_rad = np.deg2rad(fov_x), np.deg2rad(fov_y)\n",
    "# Focal lengths in pixels\n",
    "fx, fy = w / (2 * np.tan(fov_x_rad / 2)), h / (2 * np.tan(fov_y_rad / 2))\n",
    "# Intrinsic matrix\n",
    "K = np.array([[fx, 0, cx],\n",
    "                [0, fy, cy],\n",
    "                [0, 0, 1]])\n",
    "pose = pose_estimator(K, max_feature=10000, print_messages=True, size_output_img=[3000, 2400])\n",
    "\n",
    "def track_objects(image):\n",
    "    tracked_objects = tracker.process_image(image)\n",
    "    if tracked_objects:\n",
    "        # Use only the first detected car's masked image\n",
    "        first_car = tracked_objects[0]\n",
    "        masked_image = first_car[\"masked_image\"]\n",
    "\n",
    "        return pose.compute_pose(masked_image)\n",
    "\n",
    "ret, image = cap.read()\n",
    "ret, R, t, match_img = track_objects(image)\n",
    "if ret:\n",
    "    cv2.imshow('Matched image', match_img)\n",
    "ret, image = cap.read()\n",
    "idx_frame = 2\n",
    "ret, R, t, match_img = track_objects(image)\n",
    "if ret:\n",
    "    cv2.imshow('Matched image', match_img)\n",
    "\n",
    "while True:\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == ord(\"q\"):\n",
    "        break\n",
    "    elif k == ord('d'):\n",
    "        idx_frame += 1\n",
    "        ret, image = cap.read()\n",
    "        if not ret or idx_frame == -1:\n",
    "            print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "            break\n",
    "    \n",
    "        ret, R, t, match_img = track_objects(image)\n",
    "        if ret:\n",
    "            cv2.imshow('Matched image', match_img)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
